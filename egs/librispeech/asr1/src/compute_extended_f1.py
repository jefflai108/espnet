#!/usr/bin/python
import logging
import re
import sys
from collections import defaultdict
from utterance_aligner import UtteranceAligner


def get_indexs(utterance, span):
    tokens = re.split('\s+', utterance)
    span_tokens = re.split('\s+', span)
    for i in range(len(tokens) - len(span_tokens) + 1):
        match = True
        for j, span_token in enumerate(span_tokens):
            if span_token != tokens[i+j]:
                match = False
                break

        if match:
            return (i, i+len(span_tokens)-1)

    return None


def process_slot_types(string):
    return [re.split('\:', span)[0] for span in re.split('\;', string)]


def get_utterance_tp_fn_fp(aligner, transcription, hypothesis, transcription_slots, hypothesis_slots):
    transcription_slot_span_indexs = [
        get_indexs(transcription, transcription_slot) for transcription_slot in transcription_slots]
    hypothesis_slot_span_indexs = [
        get_indexs(hypothesis, hypothesis_slot) for hypothesis_slot in hypothesis_slots]

    if transcription_slot_span_indexs[0] == None:
        # assume insertion errors
        TP, FN, FP = 0, 0, 0
        alignment_trace = None

        for span in hypothesis_slot_span_indexs:
            FP += span[1] - span[0] + 1

        return TP, FN, FP, alignment_trace


    if hypothesis_slot_span_indexs[0] == None:
        # assume deletion errors
        TP, FN, FP = 0, 0, 0
        alignment_trace = None

        for span in transcription_slot_span_indexs:
            FN += span[1] - span[0] + 1

        return TP, FN, FP, alignment_trace

    transcription_slot_span_indexs = set([
        index for indexs in transcription_slot_span_indexs for index in range(indexs[0], indexs[1]+1)])
    hypothesis_slot_span_indexs = set([
        index for indexs in hypothesis_slot_span_indexs for index in range(indexs[0], indexs[1]+1)])

    _, alignment_trace = aligner.align_utterances(transcription, hypothesis)

    transcription_index, hypothesis_index = 0, 0
    TP, FN, FP = 0, 0, 0

    for alignment_symbol in alignment_trace:
        assert transcription_index <= len(re.split('\s+', transcription))
        assert hypothesis_index <= len(re.split('\s+', hypothesis))

        if alignment_symbol == UtteranceAligner.MATCH_SYM:
            if transcription_index in transcription_slot_span_indexs:
                if hypothesis_index in hypothesis_slot_span_indexs:
                    TP += 1
                else:
                    FN += 1
            else:
                if hypothesis_index in hypothesis_slot_span_indexs:
                    FP += 1
                else:
                    # slot = other
                    pass
            transcription_index += 1
            hypothesis_index += 1

        elif alignment_symbol == UtteranceAligner.DELETION_SYM:
            if transcription_index in transcription_slot_span_indexs:
                FN += 1
            else:
                # slot = other
                pass
            transcription_index += 1

        elif alignment_symbol == UtteranceAligner.INSERTION_SYM:
            if hypothesis_index in hypothesis_slot_span_indexs:
                FP += 1
            else:
                # slot = other
                pass
            hypothesis_index += 1

        elif alignment_symbol == UtteranceAligner.SUBSTITUTION_SYM:
            if transcription_index in transcription_slot_span_indexs:
                if hypothesis_index in hypothesis_slot_span_indexs:
                    FP += 1
                    FN += 1
                else:
                    FN += 1
            else:
                if hypothesis_index in hypothesis_slot_span_indexs:
                    FP += 1
                else:
                    # slot = other
                    pass
            transcription_index += 1
            hypothesis_index += 1

        else:
            raise ValueError

    return TP, FN, FP, alignment_trace


def process_test_case(test_case, aligner):
    #assert 5 <= len(test_case) <= 6
    transcription, hypothesis = test_case[0], test_case[1]
    transcription_slots = process_slot_types(test_case[2])
    hypothesis_slots = process_slot_types(test_case[3])

    TP, FN, FP, alignment_trace = get_utterance_tp_fn_fp(
        aligner, transcription, hypothesis, transcription_slots, hypothesis_slots)

    result = []
    if TP:
        result.append('{}:TP'.format(TP))
    if FN:
        result.append('{}:FN'.format(FN))
    if FP:
        result.append('{}:FP'.format(FP))

    result = ';'.join(result)
    #print(result)
    """
    f1_ground_truth = test_case[4]
    if result != f1_ground_truth:
        for line in test_case[:5]:
            print(line)
        print(result)
        print(alignment_trace)
    """
    return TP, FN, FP


def compute_char_edit_distance(token_1, token_2):
    aligner = UtteranceAligner()

    return aligner.align_utterances(
        ' '.join(list(token_1)), ' '.join(list(token_2)))[0]


def substitution_cost(alignment_matrix, i, j, tokens_1, tokens_2):
    """
    Ignore input and return 1 for all cases (insertion, deletion, substitution)
    """
    edit_distance = compute_char_edit_distance(tokens_1[i-1], tokens_2[j-1])
    #print(tokens_1[i-1], tokens_2[j-1], edit_distance/len(tokens_1[i-1]))

    if edit_distance/len(tokens_1[i-1]) < 0.7 or len(tokens_1[i-1]) < 4:
        return 1.5
    else:
        return 10

    #return 1 + min(edit_distance/len(tokens_1), 1)

def insert_delete_cost(alignment_matrix, i, j, tokens_1, tokens_2):
    edit_distance = compute_char_edit_distance(tokens_1[i-1], tokens_2[j-1])

    if edit_distance/len(tokens_1[i-1]) > 0.4:
        return 1
    else:
        return 10

    #return 1 - min(edit_distance/len(tokens_1), 1) * 0.75


def main(IBO):
    aligner = UtteranceAligner(
        substitution_cost=substitution_cost,
        insertion_cost=insert_delete_cost,
        deletion_cost=insert_delete_cost)

    test_case, TPs, FNs, FPs = [], 0, 0, 0
    utterance_files = sys.argv[1]
    with open(sys.argv[1]) as f:
        content = f.readlines()
    content = [x.strip('\n') for x in content]

    slot2F1  = defaultdict(lambda: [0,0,0]) # TPs, FNs, FPs
    noise2F1 = defaultdict(lambda: [0,0,0]) # TPs, FNs, FPs
    snr2F1   = defaultdict(lambda: [0,0,0]) # TPs, FNs, FPs
    increment = 6
    for idx in range(int(len(content)/increment)):
        test_case = content[idx*increment:idx*increment+increment]
        utt_id    = test_case[0] # 8k3-8k3084s_AirConditioner_7_SNRdb_40
        snr       = int(utt_id.split('_')[-1])
        noise     = utt_id.split('_')[1]
        #print(noise, snr)
        ref_text  = test_case[1]
        hyp_text  = test_case[2]
        ref_slots = test_case[3].split(';')
        hyp_slots = test_case[4].split(';')
        unique_ref_slots = []
        if ref_slots[0] == '':
            continue
            if hyp_slots[0] != '':
                for slot in hyp_slots:
                    hyp_slot = slot.split(':')[1]
                    slot2F1[hyp_slot][2] += 1
            continue
        for ref_slot in ref_slots:
            if IBO:
                slot_name = ref_slot.split(':')[1]
            else:
                slot_name = ref_slot.split(':')[1].split('-')[1]
            unique_ref_slots.append(slot_name)
        unique_ref_slots = set(unique_ref_slots)
        for ref_slot in unique_ref_slots:
            matched_ref_slots = [x for x in ref_slots if ref_slot in x]
            matched_hyp_slots = [x for x in hyp_slots if ref_slot in x]
            if matched_hyp_slots != []:
                sub_test_case = [ref_text, hyp_text, ';'.join(matched_ref_slots), ';'.join(matched_hyp_slots)]
            else:
                sub_test_case = [ref_text, hyp_text, ';'.join(matched_ref_slots), 'shit']
            local_TPs, local_FNs, local_FPs = process_test_case(sub_test_case, aligner)
            #print('Extended SL F1 = 2*TPs/(2*TPs + FPs + FNs) for \"{}\" is: {}'.
            #      format(hyp_text, 2*local_TPs/(2*local_TPs + local_FPs + local_FNs)))
            slot2F1[ref_slot][0]  += local_TPs
            slot2F1[ref_slot][1]  += local_FNs
            slot2F1[ref_slot][2]  += local_FPs
            noise2F1[noise][0]    += local_TPs
            noise2F1[noise][1]    += local_FNs
            noise2F1[noise][2]    += local_FPs
            snr2F1[snr][0]        += local_TPs
            snr2F1[snr][1]        += local_FNs
            snr2F1[snr][2]        += local_FPs

    all_TPs, all_FNs, all_FPs = 0, 0, 0
    for ref_slot in slot2F1.keys():
        TPs = slot2F1[ref_slot][0]
        FNs = slot2F1[ref_slot][1]
        FPs = slot2F1[ref_slot][2]
        all_TPs += TPs
        all_FNs += FNs
        all_FPs += FPs
        #print('Extended SL F1 = 2*TPs/(2*TPs + FPs + FNs) for {} is {}'.
        #      format(ref_slot, (100.0 * 2*TPs/(2*TPs + FPs + FNs))))

    print('Evaluating noise!')
    for noise in noise2F1.keys():
        TPs = noise2F1[noise][0]
        FNs = noise2F1[noise][1]
        FPs = noise2F1[noise][2]
        print('Extended SL F1 = 2*TPs/(2*TPs + FPs + FNs) for {} is {}'.
              format(noise, (100.0 * 2*TPs/(2*TPs + FPs + FNs))))

    print('Evaluating snr!')
    for snr in snr2F1.keys():
        TPs = snr2F1[snr][0]
        FNs = snr2F1[snr][1]
        FPs = snr2F1[snr][2]
        print('Extended SL F1 = 2*TPs/(2*TPs + FPs + FNs) for {} is {}'.
              format(snr, (100.0 * 2*TPs/(2*TPs + FPs + FNs))))

    print('Overall F1!')
    print('Extended SL F1 = 2*TPs/(2*TPs + FPs + FNs) for ALL is {}'.
          format((100.0 * 2*all_TPs/(2*all_TPs + all_FPs + all_FNs))))

if __name__ == '__main__':
    print('extended micro F1 score (slot)')
    main(False)
    #print('extended micro F1 score (BIO-slot)')
    #main(True)
